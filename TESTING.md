# Documentation des Tests

## ğŸ“‹ Vue d'ensemble

Ce projet utilise **pytest** pour les tests unitaires et d'intÃ©gration. Les tests sont organisÃ©s en trois catÃ©gories :

1. **Tests de PrÃ©traitement** (`test_preprocess.py`)
2. **Tests de ModÃ¨les** (`test_models.py`)
3. **Tests d'IntÃ©gration** (`test_integration_metrics.py`)

---

## ğŸ§ª Structure des Tests

### `tests/test_preprocess.py`

Teste les fonctions de prÃ©traitement et nettoyage des donnÃ©es.

#### Tests

| Test | Description | Ã‰tat |
|------|-------------|------|
| `test_preprocess_df_no_nans` | VÃ©rifie suppression des NaN | âœ… PASSED |
| `test_preprocess_from_path` | Teste chargement depuis fichier CSV | âœ… PASSED |

**Lancer :**
```bash
pytest tests/test_preprocess.py -v
```

---

### `tests/test_models.py`

Teste l'existence et structure des artefacts modÃ¨les.

#### Tests

| Test | Description | Ã‰tat |
|------|-------------|------|
| `test_model_artifact_exists` | VÃ©rifie existence `artifacts/model.joblib` | âš ï¸ Attente d'entraÃ®nement |
| `test_model_contains_model_key` | VÃ©rifie structure dict avec clÃ© 'model' | âš ï¸ Attente d'entraÃ®nement |

**Lancer :**
```bash
pytest tests/test_models.py -v
```

**Note :** Ces tests nÃ©cessitent un entraÃ®nement prÃ©alable. Lancer :
```bash
python -m src.models.train
```

---

### `tests/test_integration_metrics.py`

Teste la cohÃ©rence entre notebook et refactoring.

#### Tests

| Test | Description | Ã‰tat |
|------|-------------|------|
| `test_refactored_metrics_close_to_notebook` | VÃ©rifie MAE < 5% Ã©cart entre versions | âš ï¸ Attente de rapport |

**Lancer :**
```bash
pytest tests/test_integration_metrics.py -v
```

**Note :** NÃ©cessite un rapport `artifacts/compare_report.joblib`. Lancer :
```bash
python -m src.models.compare_pipelines
```

---

## ğŸš€ ExÃ©cution des Tests

### Tous les tests
```bash
pytest tests/ -v
```

### Un fichier spÃ©cifique
```bash
pytest tests/test_preprocess.py -v
```

### Un test spÃ©cifique
```bash
pytest tests/test_preprocess.py::test_preprocess_df_no_nans -v
```

### Avec couverture de code
```bash
pip install pytest-cov
pytest tests/ --cov=src --cov-report=html
```

### Mode silencieux (minimal output)
```bash
pytest tests/ -q
```

---

## âš™ï¸ Configuration pytest

Le fichier `pytest.ini` configure :
- **Chemins des tests** : `testpaths = tests`
- **Patterns de fichiers** : `test_*.py`
- **Verbose par dÃ©faut** : `-v`
- **Short traceback** : `--tb=short`

---

## ğŸ”§ Fixtures et Helpers

Le fichier `conftest.py` ajoute le chemin racine au Python path :

```python
import sys
import os

project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
```

Cela permet les imports relatifs comme :
```python
from src.preprocessing.preprocessor import preprocess_data
```

---

## ğŸ› DÃ©pannage

### Erreur : `ModuleNotFoundError: No module named 'src'`

**Solution :** 
```bash
# 1. VÃ©rifier que conftest.py existe dans tests/
ls tests/conftest.py

# 2. VÃ©rifier que __init__.py existent
ls src/__init__.py
ls src/preprocessing/__init__.py

# 3. Lancer pytest depuis la racine
cd /path/to/project
pytest tests/ -v
```

### Erreur : `FileNotFoundError: model.joblib`

**Solution :** EntraÃ®ner d'abord le modÃ¨le
```bash
python -m src.models.train
```

### Tests lents

**Optimisation :**
```bash
# RÃ©duire verbositÃ©
pytest tests/ -q

# ParallÃ©liser avec pytest-xdist
pip install pytest-xdist
pytest tests/ -n auto
```

---

## ğŸ“Š MÃ©triques de Couverture

Pour gÃ©nÃ©rer un rapport de couverture :
```bash
pytest tests/ --cov=src --cov-report=html
open htmlcov/index.html
```

**Objectif :** > 80% de couverture sur `src/`

---

## ğŸ”„ IntÃ©gration CI/CD

Les tests s'exÃ©cutent automatiquement sur **GitHub Actions** :

Voir `.github/workflows/ci.yml` pour dÃ©tails.

**DÃ©clencheurs :**
- Push sur `main` ou `master`
- Pull request vers `main` ou `master`

---

## ğŸ“ Bonnes Pratiques

### âœ… Ã€ faire
- Nommer les tests avec prÃ©fixe `test_`
- Utiliser des assertions explicites
- Tester cas normaux ET cas limites
- Garder tests rapides (< 1s par test)

### âŒ Ã€ Ã©viter
- Tests qui dÃ©pendent l'un de l'autre
- Tests qui modifient les donnÃ©es
- Tests sans assertions
- Tests trÃ¨s longs (> 10s)

---

## ğŸ“š Ressources

- **pytest Documentation** : https://docs.pytest.org/
- **pytest Fixtures** : https://docs.pytest.org/en/stable/how-to/fixtures.html
- **CI/CD avec pytest** : https://docs.pytest.org/en/stable/ci.html

---

**DerniÃ¨re mise Ã  jour :** Janvier 2026
